{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vi-T for Food11",
   "id": "df559d6ecf587ef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "e6487069a22b443e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install transformers pygwalker wandb\n",
    "%pip install \"git+https://github.com/b-re-w/lattent.git#egg=lattent[pytorch]\""
   ],
   "id": "3fcc1179b76a7de8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:09.238180Z",
     "start_time": "2024-12-08T09:55:02.430942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import ViTConfig, ViTModel, PretrainedConfig\n",
    "\n",
    "from lattent import TTTForCausalLM, TTTCausalLMOutput\n",
    "\n",
    "from torchvision import transforms, datasets, utils\n",
    "\n",
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple frameworks detected ['pytorch', 'jax']. Using pytorch. Set LATTENT_FRAMEWORK environment variable to override.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:11.233855Z",
     "start_time": "2024-12-08T09:55:09.248510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# WandB Initialization\n",
    "wandb.init(project=\"food11_pytorch\")"
   ],
   "id": "99804dd3727b56e1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbrew\u001B[0m (\u001B[33mbrew-research\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/shared_hdd/brew/ttt_evaluation/src/wandb/run-20241208_095510-bo6uk3o0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brew-research/food11_pytorch/runs/bo6uk3o0' target=\"_blank\">ethereal-sky-59</a></strong> to <a href='https://wandb.ai/brew-research/food11_pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/brew-research/food11_pytorch' target=\"_blank\">https://wandb.ai/brew-research/food11_pytorch</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/brew-research/food11_pytorch/runs/bo6uk3o0' target=\"_blank\">https://wandb.ai/brew-research/food11_pytorch/runs/bo6uk3o0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/brew-research/food11_pytorch/runs/bo6uk3o0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1e87eff050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "88444867b0b14178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.030657Z",
     "start_time": "2024-12-08T09:55:11.307946Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "51e70cad906b1983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 09:55:11 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    34W / 250W |   9600MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   57C    P0   138W / 250W |  15315MiB / 16280MiB |     44%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      8067      C   python                           1919MiB |\r\n",
      "|    0   N/A  N/A     15655      C   python                           1919MiB |\r\n",
      "|    0   N/A  N/A     21653      C   python                           1919MiB |\r\n",
      "|    0   N/A  N/A     30376      C   python                           1919MiB |\r\n",
      "|    0   N/A  N/A     32331      C   python                            677MiB |\r\n",
      "|    0   N/A  N/A     48935      C   ...ste_classifier/bin/python     1243MiB |\r\n",
      "|    7   N/A  N/A     16242      C   ...brew/anaconda3/bin/python    15313MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.355400Z",
     "start_time": "2024-12-08T09:55:12.044640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 6\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    #device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(f\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "#print(f\"INFO: Using device - {device}\")\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ],
   "id": "4ae5ca44145e20a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:6\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load DataSets",
   "id": "43f33be18440cabc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.379166Z",
     "start_time": "2024-12-08T09:55:12.369301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "datasets.utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class FoodImageDataset(datasets.ImageFolder):\n",
    "    download_method = datasets.utils.download_and_extract_archive\n",
    "    download_url = \"https://www.kaggle.com/api/v1/datasets/download/trolukovich/food11-image-dataset\"\n",
    "\n",
    "    def __init__(self, root: str, force_download: bool = True, train: bool = True, valid: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None):\n",
    "        self.download(root, force=force_download)\n",
    "\n",
    "        if train:\n",
    "            if valid:\n",
    "                root = path.join(root, \"validation\")\n",
    "            else:\n",
    "                root = path.join(root, \"training\")\n",
    "        else:\n",
    "            root = path.join(root, \"evaluation\")\n",
    "\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            cls.download_method(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))"
   ],
   "id": "f484d112ba3686ab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.433040Z",
     "start_time": "2024-12-08T09:55:12.426300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (224, 224)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "#feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "#vit_converter = lambda img, lb: (feature_extractor(images=img, return_tensors=\"pt\"), lb)\n",
    "\n",
    "augmenter = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])"
   ],
   "id": "50f6181b6f7f12a2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.536905Z",
     "start_time": "2024-12-08T09:55:12.475284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\", \"food11\")\n",
    "\n",
    "train_dataset = FoodImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "valid_dataset = FoodImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "test_dataset = FoodImageDataset(root=DATA_ROOT, force_download=False, train=False, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ],
   "id": "2b43129ac06b0aa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset loaded successfully. Number of samples - Train(9866), Valid(3430), Test(3347)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:12.994433Z",
     "start_time": "2024-12-08T09:55:12.585266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "id": "b215e25baf911844",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-000628bf3e8161a1BRtU87INdzsxWLSe\" style=\"height: auto\">\\n    <hea…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f58cb9fcb42048e7af3f9c3c93d25226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"000628bf3e8161a1BRtU87INdzsxWLSe\") {\n",
       "            var iframe = document.getElementById(\"gwalker-000628bf3e8161a1BRtU87INdzsxWLSe\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pygwalker.api.pygwalker.PygWalker at 0x7f1e87d769c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:13.355516Z",
     "start_time": "2024-12-08T09:55:13.166694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Valid Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "id": "f281ecfacbeb269e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-000628bf3e87aa0fxauIF7dWX5iJs2cK\" style=\"height: auto\">\\n    <hea…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfd47b87ba5d4123844e5567a220262e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"000628bf3e87aa0fxauIF7dWX5iJs2cK\") {\n",
       "            var iframe = document.getElementById(\"gwalker-000628bf3e87aa0fxauIF7dWX5iJs2cK\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pygwalker.api.pygwalker.PygWalker at 0x7f1e87c63440>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:13.725553Z",
     "start_time": "2024-12-08T09:55:13.520123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "id": "17e00e55f45d8d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-000628bf3e8d1332RmBIgfSV8bFLvZWs\" style=\"height: auto\">\\n    <hea…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fff065c0eac45cbb677ccd1a6a2e4f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"000628bf3e8d1332RmBIgfSV8bFLvZWs\") {\n",
       "            var iframe = document.getElementById(\"gwalker-000628bf3e8d1332RmBIgfSV8bFLvZWs\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pygwalker.api.pygwalker.PygWalker at 0x7f1e87cbedb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DataLoader",
   "id": "b50f6925761e5e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:13.897233Z",
     "start_time": "2024-12-08T09:55:13.893013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 64, 64, 10"
   ],
   "id": "4dd53b09a07e7b04",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:13.976603Z",
     "start_time": "2024-12-08T09:55:13.965399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ],
   "id": "c6a5748cf1442804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Number of CPU cores - 48\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:14.084901Z",
     "start_time": "2024-12-08T09:55:14.077995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)"
   ],
   "id": "f178d4118a670b57",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:55:19.463567Z",
     "start_time": "2024-12-08T09:55:14.138498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "imshow(grid_images)"
   ],
   "id": "48dc34cfd7aed946",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.053115898461357e-09..1.0000000236034394].\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Model",
   "id": "e8d5ea2a2af0c35d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "common_config = dict(\n",
    "    image_size=IMG_SIZE[0],\n",
    "    num_channels=3,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True\n",
    ")"
   ],
   "id": "250ce4a637404c37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vit_config = dict(\n",
    "    **common_config,\n",
    "    patch_size=16,\n",
    "    encoder_stride=16,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=768\n",
    ")"
   ],
   "id": "625589487aeb7839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ttt_config = dict(  # 2 GPU setting\n",
    "    **common_config,\n",
    "    patch_size=7,\n",
    "    encoder_stride=2,\n",
    "    hidden_size=768//2,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=768//2\n",
    ")"
   ],
   "id": "4418e0969f09989f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ttt_config = dict(  # 1 GPU setting\n",
    "    **common_config,\n",
    "    patch_size=4,\n",
    "    encoder_stride=2,\n",
    "    hidden_size=64,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=64,\n",
    "    scan_checkpoint_group_size=4\n",
    ")"
   ],
   "id": "189bbb1214a3df5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ttt_config = dict(  # 1 GPU setting\n",
    "    **common_config,\n",
    "    patch_size=4,           # Vi-T의 1/4\n",
    "    encoder_stride=4,       # Vi-T의 1/4\n",
    "    hidden_size=192,        # Vi-T의 1/4\n",
    "    num_hidden_layers=6,    # Vi-T의 1/2\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=192,  # Vi-T의 1/4,\n",
    "    scan_checkpoint_group_size=4\n",
    ")"
   ],
   "id": "9c32c707b01125d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ttt_config = dict(\n",
    "#     patch_size=8,          # ViT의 1/2\n",
    "#     encoder_stride=8,      # ViT의 1/2\n",
    "#     hidden_size=192,       # ViT의 1/4\n",
    "#     num_hidden_layers=6,   # ViT의 1/2\n",
    "#     num_attention_heads=8,\n",
    "#     intermediate_size=192  # ViT의 1/4\n",
    "# )"
   ],
   "id": "5a22ba82689695d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normal Vi-T",
   "id": "477f606cb03fdd7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ViTImageClassifier(nn.Module):\n",
    "    def __init__(self, config: ViTConfig, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vit = ViTModel(config=config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vit(x)\n",
    "        pooled = out.pooler_output  # [batch_size, hidden_size]\n",
    "        logits = self.fc(pooled)  # [batch_size, num_classes]\n",
    "        return logits"
   ],
   "id": "5ddcb2d583a637f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "USE_NORMAL = True\n",
    "model = ViTImageClassifier(config=ViTConfig(**vit_config), num_classes=len(train_dataset.classes))\n",
    "#model.to(device)\n",
    "model"
   ],
   "id": "f3500011660e797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TTT Vi-T",
   "id": "86186f6b0c01e3c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lattent.models import ttt_pytorch\n",
    "\n",
    "\n",
    "def scan(f, init, xs, out, checkpoint_group=0):\n",
    "    \"\"\"Optimized scan function with checkpoint groups\"\"\"\n",
    "    carry = init\n",
    "    if isinstance(xs, dict):\n",
    "        num_items = len(next(iter(xs.values())))\n",
    "    else:\n",
    "        num_items = len(xs[0])\n",
    "\n",
    "    @torch.compile(mode=\"reduce-overhead\")\n",
    "    def optimized_scan_fn(inputs, init_carry, start_idx, end_idx):\n",
    "        current_carry = init_carry\n",
    "        results = []\n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            if isinstance(inputs, dict):\n",
    "                current_input = {k: inputs[k][i] for k in inputs.keys()}\n",
    "            else:\n",
    "                current_input = inputs[i]\n",
    "\n",
    "            current_carry, y = f(current_carry, current_input)\n",
    "            results.append(y)\n",
    "\n",
    "        return current_carry, torch.stack(results)\n",
    "\n",
    "    # checkpoint_group 활용\n",
    "    if checkpoint_group > 0:\n",
    "        ckpt_every_n = max(1, num_items // checkpoint_group)\n",
    "\n",
    "        for k in range(0, num_items, ckpt_every_n):\n",
    "            end_idx = min(k + ckpt_every_n, num_items)\n",
    "            # gradient checkpointing 적용\n",
    "            carry = torch.utils.checkpoint.checkpoint(\n",
    "                optimized_scan_fn,\n",
    "                xs, carry, k, end_idx,\n",
    "                use_reentrant=False\n",
    "            )\n",
    "            # 결과를 out에 복사\n",
    "            _, results = optimized_scan_fn(xs, carry, k, end_idx)\n",
    "            for i, res in enumerate(results):\n",
    "                out[k + i] = res\n",
    "    else:\n",
    "        # checkpoint 없이 한번에 처리\n",
    "        carry, results = optimized_scan_fn(xs, carry, 0, num_items)\n",
    "        for i, res in enumerate(results):\n",
    "            out[i] = res\n",
    "\n",
    "    return carry, out\n",
    "\n",
    "\n",
    "ttt_pytorch.scan = scan"
   ],
   "id": "618557a0482213c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lattent.models import ttt_pytorch\n",
    "from lattent import TTTLinear, TTTCache\n",
    "ln_fused_l2_bwd, ln_fwd, tree_map = ttt_pytorch.ln_fused_l2_bwd, ttt_pytorch.ln_fwd, ttt_pytorch.tree_map\n",
    "\n",
    "\n",
    "class EfficientTTTLinear(TTTLinear):\n",
    "    def ttt(self, inputs, mini_batch_size, last_mini_batch_params_dict, cache_params: Optional[TTTCache] = None):\n",
    "        if mini_batch_size is None:\n",
    "            mini_batch_size = self.mini_batch_size\n",
    "\n",
    "        # in this case, we are decoding\n",
    "        if last_mini_batch_params_dict is None and cache_params is not None:\n",
    "            last_mini_batch_params_dict = cache_params.ttt_params_to_dict(self.layer_idx)\n",
    "\n",
    "        # [B, num_heads, num_mini_batch, mini_batch_size, head_dim]\n",
    "        B = inputs[\"XV\"].shape[0]\n",
    "        num_mini_batch = inputs[\"XV\"].shape[2]\n",
    "        L = inputs[\"XV\"].shape[2] * inputs[\"XV\"].shape[3]\n",
    "        device = inputs[\"XV\"].device\n",
    "        dtype = inputs[\"XV\"].dtype\n",
    "\n",
    "        use_dual_form = cache_params is None or mini_batch_size % self.mini_batch_size == 0\n",
    "\n",
    "        def compute_mini_batch(params_dict, inputs):\n",
    "            # [B, nh, f, f], nh=num_heads, f=head_dim\n",
    "            W1_init = params_dict[\"W1_states\"]\n",
    "            # [B, nh, 1, f]\n",
    "            b1_init = params_dict[\"b1_states\"]\n",
    "\n",
    "            # [B,nh,K,f], K=mini_batch_size\n",
    "            XQ_mini_batch = inputs[\"XQ\"]\n",
    "            XV_mini_batch = inputs[\"XV\"]\n",
    "            XK_mini_batch = inputs[\"XK\"]\n",
    "            # [B, nh, K, 1]\n",
    "            eta_mini_batch = inputs[\"eta\"]\n",
    "            token_eta_mini_batch = inputs[\"token_eta\"]\n",
    "            ttt_lr_eta_mini_batch = inputs[\"ttt_lr_eta\"]\n",
    "\n",
    "            X1 = XK_mini_batch\n",
    "            # [B,nh,K,f] @ [B,nh,f,f] -> [B,nh,K,f]\n",
    "            Z1 = X1 @ W1_init + b1_init\n",
    "            reconstruction_target = XV_mini_batch - XK_mini_batch\n",
    "\n",
    "            ln_weight = self.ttt_norm_weight.reshape(self.num_heads, 1, self.head_dim)\n",
    "            ln_bias = self.ttt_norm_bias.reshape(self.num_heads, 1, self.head_dim)\n",
    "            # [B,nh,K,f]\n",
    "            grad_l_wrt_Z1 = ln_fused_l2_bwd(Z1, reconstruction_target, ln_weight, ln_bias)\n",
    "\n",
    "            if use_dual_form:\n",
    "                # [B,nh,K,K]\n",
    "                Attn1 = torch.tril(XQ_mini_batch @ X1.transpose(-2, -1))\n",
    "                # [B,nh,1,f] - [B,nh,K,K] @ [B,nh,K,f] -> [B,nh,K,f]\n",
    "                b1_bar = b1_init - torch.tril(eta_mini_batch) @ grad_l_wrt_Z1\n",
    "                # [B,nh,K,f] @ [B,nh,f,f] - ([B,nh,K,1] * [B,nh,K,K]) @ [B,nh,K,f] + [B,nh,K,f]\n",
    "                Z1_bar = XQ_mini_batch @ W1_init - (eta_mini_batch * Attn1) @ grad_l_wrt_Z1 + b1_bar\n",
    "\n",
    "                last_eta_mini_batch = eta_mini_batch[:, :, -1, :, None]\n",
    "                # [B,nh,f,f] - [B,nh,f,K] @ [B,nh,K,f]\n",
    "                W1_last = W1_init - (last_eta_mini_batch * X1).transpose(-1, -2) @ grad_l_wrt_Z1\n",
    "                # [B,nh,1,f]\n",
    "                b1_last = b1_init - torch.sum(last_eta_mini_batch * grad_l_wrt_Z1, dim=-2, keepdim=True)\n",
    "                grad_W1_last = torch.zeros_like(W1_last)\n",
    "                grad_b1_last = torch.zeros_like(b1_last)\n",
    "            else:\n",
    "                ttt_lr_eta_mini_batch = torch.broadcast_to(\n",
    "                    ttt_lr_eta_mini_batch,\n",
    "                    (\n",
    "                        *ttt_lr_eta_mini_batch.shape[:2],\n",
    "                        mini_batch_size,\n",
    "                        mini_batch_size,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                # [B, nh, K, f, f]\n",
    "                grad_W1 = torch.einsum(\"bhki,bhkj->bhkij\", X1, grad_l_wrt_Z1)\n",
    "                grad_W1 = torch.einsum(\"bhnk,bhkij->bhnij\", torch.tril(ttt_lr_eta_mini_batch), grad_W1)\n",
    "                grad_W1 = grad_W1 + params_dict[\"W1_grad\"].unsqueeze(2)\n",
    "                # [B, nh, K, f]\n",
    "                grad_b1 = torch.einsum(\"bhnk,bhki->bhni\", torch.tril(ttt_lr_eta_mini_batch), grad_l_wrt_Z1)\n",
    "                grad_b1 = grad_b1 + params_dict[\"b1_grad\"]\n",
    "\n",
    "                W1_bar = W1_init.unsqueeze(2) - grad_W1 * token_eta_mini_batch.unsqueeze(-1)\n",
    "                b1_bar = b1_init - grad_b1 * token_eta_mini_batch\n",
    "\n",
    "                # [B, nh, K, 1, f] @ [B, nh, K, f, f]\n",
    "                Z1_bar = (XQ_mini_batch.unsqueeze(3) @ W1_bar).squeeze(3) + b1_bar\n",
    "\n",
    "                W1_last = W1_bar[:, :, -1]\n",
    "                b1_last = b1_bar[:, :, -1:]\n",
    "                grad_W1_last = grad_W1[:, :, -1]\n",
    "                grad_b1_last = grad_b1[:, :, -1:]\n",
    "\n",
    "            Z1_bar = ln_fwd(Z1_bar, ln_weight, ln_bias)\n",
    "            XQW_mini_batch = XQ_mini_batch + Z1_bar\n",
    "\n",
    "            last_param_dict = {\n",
    "                \"W1_states\": W1_last,\n",
    "                \"b1_states\": b1_last,\n",
    "                \"W1_grad\": grad_W1_last,\n",
    "                \"b1_grad\": grad_b1_last,\n",
    "            }\n",
    "            return last_param_dict, XQW_mini_batch\n",
    "\n",
    "        if last_mini_batch_params_dict is not None:\n",
    "            init_params_dict = last_mini_batch_params_dict\n",
    "        else:\n",
    "            init_params_dict = {\n",
    "                \"W1_states\": torch.tile(self.W1.unsqueeze(0), dims=(B, 1, 1, 1)),\n",
    "                \"b1_states\": torch.tile(self.b1.unsqueeze(0), dims=(B, 1, 1, 1)),\n",
    "            }\n",
    "            init_params_dict.update(W1_grad=torch.zeros_like(init_params_dict[\"W1_states\"]))\n",
    "            init_params_dict.update(b1_grad=torch.zeros_like(init_params_dict[\"b1_states\"]))\n",
    "\n",
    "        # [B,num_heads, num_mini_batch, mini_batch_size, f] -> [num_mini_batch, B, num_heads, mini_batch_size, f]\n",
    "        inputs = tree_map(lambda x: x.permute(2, 0, 1, 3, 4), inputs)\n",
    "\n",
    "        # Initialize batch_params_dict and XQW_last\n",
    "        batch_params_dict = init_params_dict\n",
    "        XQW_last = None\n",
    "\n",
    "        # Process mini-batches sequentially, only keeping the last result\n",
    "        for i in range(num_mini_batch - 1):\n",
    "            current_inputs = tree_map(lambda x: x[i], inputs)\n",
    "            batch_params_dict, _ = compute_mini_batch(batch_params_dict, current_inputs)\n",
    "\n",
    "        # Handle last mini-batch with padding if needed\n",
    "        last_inputs = tree_map(lambda x: x[-1], inputs)\n",
    "        current_size = last_inputs[\"XQ\"].shape[-2]\n",
    "\n",
    "        if current_size != mini_batch_size:\n",
    "            # Pad last mini-batch inputs\n",
    "            last_inputs = tree_map(\n",
    "                lambda x: F.pad(x, (0, 0, 0, mini_batch_size - current_size), mode='constant', value=0),\n",
    "                last_inputs\n",
    "            )\n",
    "\n",
    "        # Process final mini-batch\n",
    "        batch_params_dict, XQW_last = compute_mini_batch(batch_params_dict, last_inputs)\n",
    "\n",
    "        if cache_params is not None:\n",
    "            cache_params.update(batch_params_dict, self.layer_idx, L)\n",
    "\n",
    "        # Reshape final output (keeping the padding)\n",
    "        B, H, K, D = XQW_last.shape\n",
    "        XQW_last = XQW_last.permute(0, 2, 1, 3)  # [B, K, num_heads, head_dim]\n",
    "        XQW_last = XQW_last.reshape(B, K, self.width)  # [B, K, C]\n",
    "\n",
    "        return XQW_last, batch_params_dict\n",
    "\n",
    "\n",
    "class EfficientTTTBlock(ttt_pytorch.Block):\n",
    "    def __init__(self, config: ttt_pytorch.TTTConfig, layer_idx: int):\n",
    "        super().__init__(config, layer_idx)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, position_ids=None, cache_params=None):\n",
    "        if self.pre_conv:\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.conv(hidden_states, cache_params=cache_params)\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "        # TTT Layer\n",
    "        residual = hidden_states[:, -self.config.mini_batch_size:]  # 마지막 미니배치만큼만 residual connection\n",
    "        hidden_states = self.seq_norm(hidden_states)\n",
    "        hidden_states = self.seq_modeling_block(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            cache_params=cache_params,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states  # 이제 크기가 맞을 것입니다\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ffn_norm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "#ttt_pytorch.TTTLinear = EfficientTTTLinear\n",
    "#ttt_pytorch.Block = EfficientTTTBlock"
   ],
   "id": "e4ca0f75de7b2922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TTTVisionConfig(PretrainedConfig):\n",
    "    \"\"\"Vision TTT configuration.\"\"\"\n",
    "\n",
    "    model_type = \"vision_ttt\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_channels=3,\n",
    "            num_classes=1000,\n",
    "            hidden_size=768,\n",
    "            intermediate_size=3072,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            hidden_act=\"gelu\",\n",
    "            initializer_range=0.02,\n",
    "            rms_norm_eps=1e-6,\n",
    "            pretraining_tp=1,\n",
    "            use_cache=True,\n",
    "            rope_theta=10000.0,\n",
    "            mini_batch_size=16,\n",
    "            use_gate=False,\n",
    "            share_qk=False,\n",
    "            ttt_layer_type=\"linear\",\n",
    "            ttt_base_lr=1.0,\n",
    "            pre_conv=True,\n",
    "            conv_kernel=4,\n",
    "            scan_checkpoint_group_size=0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = num_classes\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pretraining_tp = pretraining_tp\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "\n",
    "        self.use_gate = use_gate\n",
    "        self.share_qk = share_qk\n",
    "        self.ttt_layer_type = ttt_layer_type\n",
    "        self.ttt_base_lr = ttt_base_lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "        self.pre_conv = pre_conv\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.scan_checkpoint_group_size = scan_checkpoint_group_size\n",
    "\n",
    "        # Vision-specific attributes\n",
    "        self.num_patches = (image_size // patch_size) ** 2"
   ],
   "id": "274a5c074bc5ac84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts images into patches and projects them into the model dimension.\"\"\"\n",
    "    def __init__(self, config: TTTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_patches = config.num_patches\n",
    "        self.num_channels = config.num_channels\n",
    "\n",
    "        # Patch splitting and flattening\n",
    "        patch_dim = self.num_channels * self.patch_size * self.patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # Split image into patches and flatten\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            # Project to hidden dimension\n",
    "            nn.Linear(patch_dim, config.hidden_size),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Learnable position embeddings for patches\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, config.hidden_size))\n",
    "\n",
    "        # [CLS] token embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        B = pixel_values.shape[0]\n",
    "\n",
    "        # Convert image to patches -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.to_patch_embedding(pixel_values)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        return self.dropout(x)"
   ],
   "id": "992d9de200635acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers.utils import ModelOutput\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class TTTForVisionCausalLM(TTTForCausalLM):\n",
    "    config_class = TTTVisionConfig\n",
    "\n",
    "    def __init__(self, config: TTTVisionConfig):\n",
    "        super().__init__(config)\n",
    "        self.patch_embed = PatchEmbedding(config)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,  # [batch_size, seq_len]\n",
    "            attention_mask: Optional[torch.Tensor] = None,  # [batch_size, seq_len]\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,  # [batch_size, seq_len, hidden_size]\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,  # [batch_size, channels, height, width]\n",
    "            cache_params: Optional[Any] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            **kwargs\n",
    "    ) -> Union[Tuple, TTTCausalLMOutput]:\n",
    "        batch_size = None\n",
    "        if pixel_values is not None:\n",
    "            batch_size = pixel_values.shape[0]\n",
    "        elif input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"No valid input provided to determine batch size\")\n",
    "\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        final_attention_mask = None\n",
    "        final_embeds = None\n",
    "\n",
    "        # 이미지 처리\n",
    "        if pixel_values is not None:\n",
    "            # [batch_size, num_patches + 1, hidden_size]\n",
    "            image_embeds = self.patch_embed(pixel_values)\n",
    "            image_attention_mask = torch.ones(\n",
    "                (batch_size, image_embeds.shape[1]),\n",
    "                dtype=torch.long,\n",
    "                device=image_embeds.device\n",
    "            )\n",
    "            final_embeds = image_embeds  # [batch_size, num_patches + 1, hidden_size]\n",
    "            final_attention_mask = image_attention_mask  # [batch_size, num_patches + 1]\n",
    "\n",
    "        # 텍스트 처리 (선택적)\n",
    "        if input_ids is not None or inputs_embeds is not None:\n",
    "            if inputs_embeds is None:\n",
    "                # [batch_size, seq_len] -> [batch_size, seq_len, hidden_size]\n",
    "                inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "            text_attention_mask = attention_mask if attention_mask is not None else torch.ones(\n",
    "                (batch_size, inputs_embeds.shape[1]),\n",
    "                dtype=torch.long,\n",
    "                device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "            # 이미지가 있는 경우 텍스트와 연결\n",
    "            if final_embeds is not None:\n",
    "                # [batch_size, num_patches + 1 + seq_len, hidden_size]\n",
    "                final_embeds = torch.cat([final_embeds, inputs_embeds], dim=1)\n",
    "                # [batch_size, num_patches + 1 + seq_len]\n",
    "                final_attention_mask = torch.cat([final_attention_mask, text_attention_mask], dim=1)\n",
    "            else:\n",
    "                final_embeds = inputs_embeds  # [batch_size, seq_len, hidden_size]\n",
    "                final_attention_mask = text_attention_mask  # [batch_size, seq_len]\n",
    "\n",
    "        if final_embeds is None:\n",
    "            raise ValueError(\"Either pixel_values or input_ids/inputs_embeds must be provided\")\n",
    "\n",
    "        # TTT 모델 실행\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=final_attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=final_embeds,\n",
    "            cache_params=cache_params,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        # [batch_size, total_seq_len, hidden_size]\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # 로짓 계산\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)  # [batch_size, total_seq_len, vocab_size]\n",
    "        else:\n",
    "            # [batch_size, total_seq_len, vocab_size]\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        # Loss 계산\n",
    "        loss = None\n",
    "        if labels is not None and (input_ids is not None or inputs_embeds is not None):\n",
    "            text_start = self.config.num_patches + 1 if pixel_values is not None else 0\n",
    "            # [batch_size, seq_len - 1, vocab_size]\n",
    "            shift_logits = logits[:, text_start:-1, :].contiguous()\n",
    "            # [batch_size, seq_len - 1]\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "            shift_labels = shift_labels.view(-1)  # [batch_size * (seq_len - 1)]\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return TTTCausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,  # [batch_size, total_seq_len, vocab_size]\n",
    "            cache_params=outputs.cache_params,\n",
    "            hidden_states=outputs.hidden_states\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            cache_params: Optional[Any] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        model_inputs = {}\n",
    "\n",
    "        # 캐시가 있는 경우 마지막 토큰만 처리\n",
    "        if cache_params is not None:\n",
    "            if input_ids is not None:\n",
    "                input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask[:, -1].unsqueeze(-1)\n",
    "            if inputs_embeds is not None:\n",
    "                inputs_embeds = inputs_embeds[:, -1:, :]\n",
    "\n",
    "        # 입력 설정\n",
    "        if inputs_embeds is not None and cache_params is None:\n",
    "            model_inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "        elif input_ids is not None:\n",
    "            model_inputs[\"input_ids\"] = input_ids\n",
    "\n",
    "        # 첫 forward pass에서만 이미지 처리\n",
    "        if pixel_values is not None and cache_params is None:\n",
    "            model_inputs[\"pixel_values\"] = pixel_values\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"cache_params\": cache_params,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def _update_model_kwargs_for_generation(\n",
    "            self,\n",
    "            outputs: ModelOutput,\n",
    "            model_kwargs: Dict[str, Any],\n",
    "            **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        model_kwargs[\"cache_params\"] = outputs.get(\"cache_params\", None)\n",
    "\n",
    "        if \"attention_mask\" in model_kwargs:\n",
    "            attention_mask = model_kwargs[\"attention_mask\"]\n",
    "            if attention_mask is not None:\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "        # 첫 번째 forward pass 이후에는 이미지 처리 불필요\n",
    "        if \"pixel_values\" in model_kwargs:\n",
    "            del model_kwargs[\"pixel_values\"]\n",
    "\n",
    "        return model_kwargs"
   ],
   "id": "aa34528aea2c2c8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "USE_NORMAL = False\n",
    "model = TTTForVisionCausalLM(config=TTTVisionConfig(\n",
    "    **ttt_config, num_classes=len(train_dataset.classes), mini_batch_size=56\n",
    "))\n",
    "#model = nn.DataParallel(model, device_ids=[DEVICE_NUM, DEVICE_NUM+1])\n",
    "model.to(device)"
   ],
   "id": "9c87884c1ea1829b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "2f914f6148a87ae7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    train_line, = ax.plot(train_losses, label=\"Train Loss\", color=\"purple\")\n",
    "    valid_line, = ax.plot(valid_losses, label=\"Valid Loss\", color=\"red\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(train_loss=None, valid_loss=None):\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss)\n",
    "        if valid_loss is not None:\n",
    "            valid_losses.append(valid_loss)\n",
    "        train_line.set_ydata(train_losses)\n",
    "        train_line.set_xdata(range(len(train_losses)))\n",
    "        valid_line.set_ydata(valid_losses)\n",
    "        valid_line.set_xdata(range(len(valid_losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "id": "6d22e15544d20781",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ],
   "id": "967ecd5b0e916b90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_CACHE = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "id": "42dd00e380308053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normal Vi-T",
   "id": "ca59696e6896b548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "if not USE_NORMAL:\n",
    "    raise ValueError(\"Model is not set to Normal Vi-T. Please set USE_NORMAL to True.\")\n",
    "\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_acc, train_loss = 0, 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() / train_length\n",
    "            corrects = (torch.max(outputs, 1)[1] == targets.data).sum()\n",
    "            train_acc += corrects / len(train_dataset)\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'Acc': corrects/len(inputs)*100, 'Loss': loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:2}/{train_length}], Acc: {corrects/len(inputs):.6%}, Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {train_acc:.6%}, Loss: {train_loss:.6f}\", end=\"\")\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / (len(inputs) * valid_length)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        update(train_loss=train_loss, valid_loss=val_loss)\n",
    "        wandb.log({'Train Acc': train_acc*100, 'Train Loss': train_loss, 'Val Acc': val_acc*100, 'Val Loss': val_loss})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {train_acc:.6%}, Loss: {train_loss:.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "id": "581a23258c46931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    import os\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"normal_vit_model.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "8a33998508f5da6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TTT Vi-T",
   "id": "3e3e79da650eb914"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "    tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(pixel_values=inputs, labels=targets, use_cache=False).logits[:, -1, :]\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            if MAX_GRAD_NORM > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'Acc': avg(train_acc)*100, 'Loss': avg(train_loss)})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:2}/{train_length}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}\", end=\"\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        cache_params = None  # Save the training cache state\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                if USE_CACHE:\n",
    "                    outputs = model(\n",
    "                        pixel_values=inputs,\n",
    "                        labels=targets,  # Pass labels to compute model's internal loss\n",
    "                        cache_params=cache_params,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = model(pixel_values=inputs, labels=targets, use_cache=False)\n",
    "                outputs, cache_params = outputs.logits[:, -1, :], outputs.cache_params\n",
    "\n",
    "                val_loss.append(criterion(outputs, targets).item())\n",
    "                val_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        update(train_loss=avg(train_loss), valid_loss=avg(val_loss))\n",
    "        wandb.log({'Train Acc': avg(train_acc)*100, 'Train Loss': avg(train_loss), 'Val Acc': avg(val_acc)*100, 'Val Loss': avg(val_loss)})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}, Valid Acc: {avg(val_acc):.6%}, Valid Loss: {avg(val_loss):.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "id": "ce7fd351c87211b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    import os\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"ttt_vit_model.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "f2a40cce237a6e53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Evaluation",
   "id": "7762d126105c4809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Model\n",
    "model_id = \"normal_vit_model\"\n",
    "\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "id": "435a4c75ebeb969d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "id": "9faf5249a208f03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(pixel_values=inputs, labels=targets, use_cache=False)\n",
    "        _, preds = torch.max(outputs.logits[:, -1, :], 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "id": "c05d535d69419f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9950465b4726834f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
