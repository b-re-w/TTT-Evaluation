{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Performance Verification and Transferability Evaluation of TTT Layer\n",
    "\n",
    ":reference: https://github.com/test-time-training/ttt-lm-pytorch\n",
    "\n",
    ":suggesting paper: https://arxiv.org/abs/2407.04620"
   ],
   "id": "896c98fb837579c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "d7adcd88590b5c6f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.notebook import tqdm"
   ],
   "id": "f18cca067a9d50a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "df53b2b0752eff21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:04.358088Z",
     "start_time": "2024-08-08T09:40:03.896990Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "8c8544dce3e714e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 09:40:03 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    34W / 250W |   5661MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    31W / 250W |   2436MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     32092      C   ...on03/anaconda3/bin/python     5659MiB |\r\n",
      "|    6   N/A  N/A     33459      C   ...ol11/anaconda3/bin/python     1485MiB |\r\n",
      "|    6   N/A  N/A     33475      C   ...ol11/anaconda3/bin/python      949MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:04.428895Z",
     "start_time": "2024-08-08T09:40:04.361237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set CUDA Device\n",
    "device_num = 1\n",
    "\n",
    "if torch.cuda.is_available() and device_num != -1:\n",
    "    torch.cuda.set_device(device_num)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_num = -1  # cpu\n",
    "print(f\"INFO: Using device - {device}:{device_num}\")"
   ],
   "id": "2fece33978cf8617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. From Quick Start Example\n",
    "\n",
    "[**Paper**](https://arxiv.org/abs/2407.04620)\n",
    "| [**JAX Codebase**](https://github.com/test-time-training/ttt-lm-jax)\n",
    "| [**Setup**](#environment-setup)\n",
    "| [**Quick Start**](#quick-start)\n",
    "| [**Inference Benchmark**](https://github.com/test-time-training/ttt-lm-kernels)\n",
    "\n",
    "This is the official PyTorch model implementation of [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/abs/2407.04620). \n",
    "We **do not recommend training** with this codebase, because it is written in pure PyTorch without any systems optimization, so training will be slow, especially when the per-device batch size is small.\n",
    "\n",
    "\n",
    "For training code, or to replicate results from our paper, please view our [JAX codebase](https://github.com/test-time-training/ttt-lm-jax). For inference kernels, or to replicate speed benchmarks from our paper, please view our [kernel implementations](https://github.com/test-time-training/ttt-lm-kernels).\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers\n",
    "have linear complexity, but their performance in long context is limited by the expressive power\n",
    "of their hidden state. We propose a new class of sequence modeling layers with linear complexity\n",
    "and an expressive hidden state. The key idea is to make the hidden state a machine learning\n",
    "model itself, and the update rule a step of self-supervised learning. \n",
    "\n",
    "Since the hidden state is updated by training even on test sequences, our layers are called **Test-Time Training (TTT) layers**.\n",
    "We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model\n",
    "and a two-layer MLP respectively. "
   ],
   "id": "40030e2119a011b6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:04.819399Z",
     "start_time": "2024-08-08T09:40:04.430652Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from ttt.lm.pytorch import TTTForCausalLM, TTTConfig, TTT_STANDARD_CONFIGS\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:04.825654Z",
     "start_time": "2024-08-08T09:40:04.821526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Quantization Config\n",
    "DO_QUANTIZATION = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "original_model_params = dict(\n",
    "    low_cpu_mem_usage=True, quantization_config=bnb_config, device_map=device.type\n",
    ") if DO_QUANTIZATION else dict(low_cpu_mem_usage=True, device_map=device.type)"
   ],
   "id": "775cba1bf05f6f8e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:05.923838Z",
     "start_time": "2024-08-08T09:40:05.439006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Common Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "6bc2b5e9d7634044",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:05.933714Z",
     "start_time": "2024-08-08T09:40:05.925894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing a TTT ttt-1b style configuration\n",
    "# configuration = TTTConfig(**TTT_STANDARD_CONFIGS['1b']) is equivalent to the following\n",
    "configuration = TTTConfig()\n",
    "configuration"
   ],
   "id": "b02c053a80390bb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"conv_kernel\": 4,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5504,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mini_batch_size\": 16,\n",
       "  \"model_type\": \"ttt\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pre_conv\": false,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"scan_checkpoint_group_size\": 0,\n",
       "  \"share_qk\": false,\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"ttt_base_lr\": 1.0,\n",
       "  \"ttt_layer_type\": \"linear\",\n",
       "  \"use_cache\": false,\n",
       "  \"use_gate\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Arch Comparison",
   "id": "c5585d62e6376b54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:36:05.247044Z",
     "start_time": "2024-08-08T09:35:40.319375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing a model from the ttt-1b style configuration\n",
    "model = TTTForCausalLM(configuration)\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "17547ffd0ed43201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTForCausalLM(\n",
       "  (model): TTTModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (seq_modeling_block): TTTLinear(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (post_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (mlp): SwiGluMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (seq_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:40:18.601268Z",
     "start_time": "2024-08-08T09:40:10.473365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For comparison with the normal llm model architecture\n",
    "original = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=\"./.cache\", **original_model_params)\n",
    "original.eval()"
   ],
   "id": "9d1ce5eb82c9a05b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f5523de5ba04954b0f60fd0a7b484b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU \u0001 has a total capacity of 15.90 GiB of which 113.75 MiB is free. Including non-PyTorch memory, this process has 15.79 GiB memory in use. Of the allocated memory 15.15 GiB is allocated by PyTorch, and 1.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# For comparison with the normal llm model architecture\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m original \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, cache_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./.cache\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moriginal_model_params)\n\u001B[1;32m      3\u001B[0m original\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m/shared_hdd/brew/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    566\u001B[0m     )\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m/shared_hdd/brew/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3941\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3931\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3932\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   3934\u001B[0m     (\n\u001B[1;32m   3935\u001B[0m         model,\n\u001B[1;32m   3936\u001B[0m         missing_keys,\n\u001B[1;32m   3937\u001B[0m         unexpected_keys,\n\u001B[1;32m   3938\u001B[0m         mismatched_keys,\n\u001B[1;32m   3939\u001B[0m         offload_index,\n\u001B[1;32m   3940\u001B[0m         error_msgs,\n\u001B[0;32m-> 3941\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_load_pretrained_model(\n\u001B[1;32m   3942\u001B[0m         model,\n\u001B[1;32m   3943\u001B[0m         state_dict,\n\u001B[1;32m   3944\u001B[0m         loaded_state_dict_keys,  \u001B[38;5;66;03m# XXX: rename?\u001B[39;00m\n\u001B[1;32m   3945\u001B[0m         resolved_archive_file,\n\u001B[1;32m   3946\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m   3947\u001B[0m         ignore_mismatched_sizes\u001B[38;5;241m=\u001B[39mignore_mismatched_sizes,\n\u001B[1;32m   3948\u001B[0m         sharded_metadata\u001B[38;5;241m=\u001B[39msharded_metadata,\n\u001B[1;32m   3949\u001B[0m         _fast_init\u001B[38;5;241m=\u001B[39m_fast_init,\n\u001B[1;32m   3950\u001B[0m         low_cpu_mem_usage\u001B[38;5;241m=\u001B[39mlow_cpu_mem_usage,\n\u001B[1;32m   3951\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[1;32m   3952\u001B[0m         offload_folder\u001B[38;5;241m=\u001B[39moffload_folder,\n\u001B[1;32m   3953\u001B[0m         offload_state_dict\u001B[38;5;241m=\u001B[39moffload_state_dict,\n\u001B[1;32m   3954\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mtorch_dtype,\n\u001B[1;32m   3955\u001B[0m         hf_quantizer\u001B[38;5;241m=\u001B[39mhf_quantizer,\n\u001B[1;32m   3956\u001B[0m         keep_in_fp32_modules\u001B[38;5;241m=\u001B[39mkeep_in_fp32_modules,\n\u001B[1;32m   3957\u001B[0m         gguf_path\u001B[38;5;241m=\u001B[39mgguf_path,\n\u001B[1;32m   3958\u001B[0m     )\n\u001B[1;32m   3960\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   3961\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m/shared_hdd/brew/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4415\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001B[0m\n\u001B[1;32m   4411\u001B[0m                 set_module_tensor_to_device(\n\u001B[1;32m   4412\u001B[0m                     model_to_load, key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m*\u001B[39mparam\u001B[38;5;241m.\u001B[39msize(), dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   4413\u001B[0m                 )\n\u001B[1;32m   4414\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4415\u001B[0m         new_error_msgs, offload_index, state_dict_index \u001B[38;5;241m=\u001B[39m _load_state_dict_into_meta_model(\n\u001B[1;32m   4416\u001B[0m             model_to_load,\n\u001B[1;32m   4417\u001B[0m             state_dict,\n\u001B[1;32m   4418\u001B[0m             loaded_keys,\n\u001B[1;32m   4419\u001B[0m             start_prefix,\n\u001B[1;32m   4420\u001B[0m             expected_keys,\n\u001B[1;32m   4421\u001B[0m             device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[1;32m   4422\u001B[0m             offload_folder\u001B[38;5;241m=\u001B[39moffload_folder,\n\u001B[1;32m   4423\u001B[0m             offload_index\u001B[38;5;241m=\u001B[39moffload_index,\n\u001B[1;32m   4424\u001B[0m             state_dict_folder\u001B[38;5;241m=\u001B[39mstate_dict_folder,\n\u001B[1;32m   4425\u001B[0m             state_dict_index\u001B[38;5;241m=\u001B[39mstate_dict_index,\n\u001B[1;32m   4426\u001B[0m             dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m   4427\u001B[0m             hf_quantizer\u001B[38;5;241m=\u001B[39mhf_quantizer,\n\u001B[1;32m   4428\u001B[0m             is_safetensors\u001B[38;5;241m=\u001B[39mis_safetensors,\n\u001B[1;32m   4429\u001B[0m             keep_in_fp32_modules\u001B[38;5;241m=\u001B[39mkeep_in_fp32_modules,\n\u001B[1;32m   4430\u001B[0m             unexpected_keys\u001B[38;5;241m=\u001B[39munexpected_keys,\n\u001B[1;32m   4431\u001B[0m         )\n\u001B[1;32m   4432\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_error_msgs\n\u001B[1;32m   4433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4434\u001B[0m     \u001B[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001B[39;00m\n",
      "File \u001B[0;32m/shared_hdd/brew/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:936\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001B[0m\n\u001B[1;32m    925\u001B[0m     state_dict_index \u001B[38;5;241m=\u001B[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m    927\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m is_quantized\n\u001B[1;32m    928\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m hf_quantizer\u001B[38;5;241m.\u001B[39mrequires_parameters_quantization)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m ):\n\u001B[1;32m    935\u001B[0m     \u001B[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001B[39;00m\n\u001B[0;32m--> 936\u001B[0m     set_module_tensor_to_device(model, param_name, param_device, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mset_module_kwargs)\n\u001B[1;32m    937\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    938\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001B[0;32m/shared_hdd/brew/anaconda3/lib/python3.12/site-packages/accelerate/utils/modeling.py:416\u001B[0m, in \u001B[0;36mset_module_tensor_to_device\u001B[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001B[0m\n\u001B[1;32m    414\u001B[0m             module\u001B[38;5;241m.\u001B[39m_parameters[tensor_name] \u001B[38;5;241m=\u001B[39m param_cls(new_value, requires_grad\u001B[38;5;241m=\u001B[39mold_value\u001B[38;5;241m.\u001B[39mrequires_grad)\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 416\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    418\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(value, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU \u0001 has a total capacity of 15.90 GiB of which 113.75 MiB is free. Including non-PyTorch memory, this process has 15.79 GiB memory in use. Of the allocated memory 15.15 GiB is allocated by PyTorch, and 1.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Output Comparison",
   "id": "c6f277d5a52afab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:38:50.921652Z",
     "start_time": "2024-08-08T09:38:50.908969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Greeting from TTT! Please generate a text for me only in Korean.\"\n",
    "\n",
    "inf_params = dict(\n",
    "    input_ids=tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ],
   "id": "67258248b9b23674",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:38:55.981171Z",
     "start_time": "2024-08-08T09:38:51.377411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference using TTT\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inf_params)\n",
    "    print(*tokenizer.batch_decode(out_ids, skip_special_tokens=True))"
   ],
   "id": "566b1acb14c68f55",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting from TTT! Please generate a text for me only in Korean.ühr opport Ton--------UTEinners ultimately FF beyondലnete Nord чу NC Login)`, reasonableívían]],Per MS Einwo abolt nearcost Abd--+ Whitearlo vector gab\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inference using the Original Model\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inf_params)\n",
    "    print(*tokenizer.batch_decode(out_ids, skip_special_tokens=True))"
   ],
   "id": "d65569112187b32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. [Vision][PyTorch] Training Speed & Accuracy Comparison (1)\n",
    "    - replace attention layer with TTT layer from ResNet-like model\n",
    "    - start from random initialized weights"
   ],
   "id": "2ce8cb1e49e01e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6667ddee36570a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. [Audio][PyTorch] Training Speed & Accuracy Comparison (2)\n",
    "    - replace attention layer with TTT layer from ResNet-like model\n",
    "    - start from random initialized weights\n",
    "    - evaluate the music genre classification performance (using dataset below)\n",
    "    - https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71544"
   ],
   "id": "a5fc8c7bd0b564b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9c59e283c4342af1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. [Vision][JAX] Training Speed & Accuracy Comparison (3)\n",
    "    - replace attention layer with TTT layer from Vi-T model"
   ],
   "id": "86f85b4c10dfc617"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3-1. Start from random initialized weights",
   "id": "8d9fe8eb6a6cd94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:44:18.637251Z",
     "start_time": "2024-08-08T09:44:16.709857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "jax.default_backend()"
   ],
   "id": "7201666585ccba47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3-2. Use pretrained weights",
   "id": "c3b289bae238eb88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4596f8396f24e1e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. [Vision][JAX] Weight Transferability Evaluation (1)\n",
    "    - replace attention layer with TTT layer from a Pre-Trained Vi-T model and transfer weights"
   ],
   "id": "2ae6d55ec9903482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60f62b1c8d3a5bdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. [NLP][JAX] Weight Transferability Evaluation (2)\n",
    "    - replace attention layer with TTT layer from a Llama3.1 model and transfer weights\n",
    "    - evaluate the performance via perplexity / likelihood"
   ],
   "id": "503429511e40ce40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd75e5e9c16c0cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. [NLP][JAX] Weight Transferability Evaluation (3)\n",
    "    - replace attention layer with TTT layer from a Llama3.1 model and transfer weights\n",
    "    - evaluate the sentence domain classification performance (using dataset below)\n",
    "    - https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71633"
   ],
   "id": "dc1ab36893a834d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "19740fedf5e538ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "150afa9f86038cd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
