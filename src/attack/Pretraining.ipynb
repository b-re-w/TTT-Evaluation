{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vi-T for CIFAR-10",
   "id": "df559d6ecf587ef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "e6487069a22b443e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install transformers wandb\n",
    "%pip install \"git+https://github.com/b-re-w/lattent.git#egg=lattent[pytorch]\""
   ],
   "id": "3fcc1179b76a7de8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-27T10:44:27.493195Z",
     "start_time": "2025-04-27T10:44:17.822888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import path, mkdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ViTConfig, ViTModel, PretrainedConfig\n",
    "from lattent import TTTForCausalLM, TTTCausalLMOutput\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Huggingface login",
   "id": "aaebb5e60434fc75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:44:28.460257Z",
     "start_time": "2025-04-27T10:44:28.455866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment out the line below when you need to login to Huggingface\n",
    "#!huggingface-cli login"
   ],
   "id": "96406df8351e29f7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "88444867b0b14178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:44:30.253014Z",
     "start_time": "2025-04-27T10:44:28.955284Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "51e70cad906b1983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Sun Apr 27 10:44:29 2025       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0              33W / 250W |   2494MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   36C    P0              30W / 250W |    260MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0              27W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0              25W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0              25W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0              27W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0              25W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   34C    P0              25W / 250W |      6MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:44:30.676956Z",
     "start_time": "2025-04-27T10:44:30.603905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ],
   "id": "4ae5ca44145e20a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load DataSets",
   "id": "43f33be18440cabc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:44:31.416103Z",
     "start_time": "2025-04-27T10:44:30.685270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pretraining.datasets import (\n",
    "    ImageNet1K, CIFAR100, CIFAR10, DatasetHolder,\n",
    "    IMAGENET1KConfig, CIFAR100Config, CIFAR10Config\n",
    ")"
   ],
   "id": "7b41127ff906823e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-27T10:44:31.942375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "IMAGENETs = DatasetHolder(\n",
    "    config=IMAGENET1KConfig,\n",
    "    train=ImageNet1K(\n",
    "        root=DATA_ROOT, force_download=False, train=True, transform=IMAGENET1KConfig.augmentation\n",
    "    ),\n",
    "    valid=ImageNet1K(\n",
    "        root=DATA_ROOT, force_download=False, valid=True, transform=IMAGENET1KConfig.resizer\n",
    "    ),\n",
    "    test=ImageNet1K(\n",
    "        root=DATA_ROOT, force_download=False, train=False, transform=IMAGENET1KConfig.resizer\n",
    "    )\n",
    ")\n",
    "IMAGENETs.split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {IMAGENETs}\")\n",
    "\n",
    "CIFAR100s = DatasetHolder(\n",
    "    config=CIFAR100Config,\n",
    "    train=CIFAR100(\n",
    "        root=DATA_ROOT, download=True, train=True, transform=CIFAR100Config.augmentation\n",
    "    ),\n",
    "    test=CIFAR100(\n",
    "        root=DATA_ROOT, download=True, train=False, transform=CIFAR100Config.resizer\n",
    "    )\n",
    ")\n",
    "CIFAR100s.split_train_valid()\n",
    "CIFAR100s.split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {CIFAR100s}\")\n",
    "\n",
    "CIFAR10s = DatasetHolder(\n",
    "    config=CIFAR10Config,\n",
    "    train=CIFAR10(\n",
    "        root=DATA_ROOT, download=True, train=True, transform=CIFAR10Config.augmentation\n",
    "    ),\n",
    "    test=CIFAR10(\n",
    "        root=DATA_ROOT, download=True, train=False, transform=CIFAR10Config.resizer\n",
    "    )\n",
    ")\n",
    "CIFAR10s.split_train_valid()\n",
    "CIFAR10s.split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {CIFAR10s}\")"
   ],
   "id": "327d472ec0525e33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'ILSVRC/imagenet-1k' dataset from huggingface to data/imagenet-1k/train...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_images_0.tar.gz:   0%|          | 0.00/29.1G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6391460961754c7582fc70bb82e6cf67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CHOSEN_DATASET = IMAGENETs\n",
    "\n",
    "train_dataset = CHOSEN_DATASET.train\n",
    "valid_dataset = CHOSEN_DATASET.valid\n",
    "test_dataset = CHOSEN_DATASET.test"
   ],
   "id": "6dfaaf862a9a0c10",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DataLoader",
   "id": "b50f6925761e5e63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 512, 200, 200"
   ],
   "id": "4dd53b09a07e7b04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ],
   "id": "c6a5748cf1442804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_loader.show_sample_grid()",
   "id": "f178d4118a670b57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Model",
   "id": "e8d5ea2a2af0c35d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "common_config = dict(\n",
    "    image_size=IMG_SIZE[0],\n",
    "    num_channels=3,\n",
    "    patch_size=8,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=768,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True,\n",
    "    encoder_stride=16\n",
    ")"
   ],
   "id": "250ce4a637404c37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normal Vi-T",
   "id": "477f606cb03fdd7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ViTImageClassifier(nn.Module):\n",
    "    def __init__(self, config: ViTConfig, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vit = ViTModel(config=config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vit(x)\n",
    "        pooled = out.pooler_output  # [batch_size, hidden_size]\n",
    "        logits = self.fc(pooled)  # [batch_size, num_classes]\n",
    "        return logits"
   ],
   "id": "5ddcb2d583a637f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "model = ViTImageClassifier(config=ViTConfig(**common_config), num_classes=len(train_dataset.classes))\n",
    "#model.to(device)\n",
    "model"
   ],
   "id": "f3500011660e797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TTT Vi-T",
   "id": "86186f6b0c01e3c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TTTVisionConfig(PretrainedConfig):\n",
    "    \"\"\"Vision TTT configuration.\"\"\"\n",
    "\n",
    "    model_type = \"vision_ttt\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_channels=3,\n",
    "            num_classes=1000,\n",
    "            hidden_size=768,\n",
    "            intermediate_size=3072,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            hidden_act=\"gelu\",\n",
    "            initializer_range=0.02,\n",
    "            rms_norm_eps=1e-6,\n",
    "            pretraining_tp=1,\n",
    "            use_cache=True,\n",
    "            mini_batch_size=16,\n",
    "            use_gate=False,\n",
    "            share_qk=False,\n",
    "            ttt_layer_type=\"linear\",\n",
    "            ttt_base_lr=1.0,\n",
    "            pre_conv=True,\n",
    "            conv_kernel=4,\n",
    "            scan_checkpoint_group_size=0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = num_classes\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pretraining_tp = pretraining_tp\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.use_gate = use_gate\n",
    "        self.share_qk = share_qk\n",
    "        self.ttt_layer_type = ttt_layer_type\n",
    "        self.ttt_base_lr = ttt_base_lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "        self.pre_conv = pre_conv\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.scan_checkpoint_group_size = scan_checkpoint_group_size\n",
    "\n",
    "        # Vision-specific attributes\n",
    "        self.num_patches = (image_size // patch_size) ** 2"
   ],
   "id": "274a5c074bc5ac84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts images into patches and projects them into the model dimension.\"\"\"\n",
    "    def __init__(self, config: TTTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_patches = config.num_patches\n",
    "        self.num_channels = config.num_channels\n",
    "\n",
    "        # Patch splitting and flattening\n",
    "        patch_dim = self.num_channels * self.patch_size * self.patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # Split image into patches and flatten\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            # Project to hidden dimension\n",
    "            nn.Linear(patch_dim, config.hidden_size),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Learnable position embeddings for patches\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, config.hidden_size))\n",
    "\n",
    "        # [CLS] token embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        B = pixel_values.shape[0]\n",
    "\n",
    "        # Convert image to patches -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.to_patch_embedding(pixel_values)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        return self.dropout(x)"
   ],
   "id": "992d9de200635acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers.utils import ModelOutput\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class TTTForVisionCausalLM(TTTForCausalLM):\n",
    "    config_class = TTTVisionConfig\n",
    "\n",
    "    def __init__(self, config: TTTVisionConfig):\n",
    "        super().__init__(config)\n",
    "        self.patch_embed = PatchEmbedding(config)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,  # [batch_size, seq_len]\n",
    "            attention_mask: Optional[torch.Tensor] = None,  # [batch_size, seq_len]\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,  # [batch_size, seq_len, hidden_size]\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,  # [batch_size, channels, height, width]\n",
    "            cache_params: Optional[Any] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            **kwargs\n",
    "    ) -> Union[Tuple, TTTCausalLMOutput]:\n",
    "        batch_size = None\n",
    "        if pixel_values is not None:\n",
    "            batch_size = pixel_values.shape[0]\n",
    "        elif input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"No valid input provided to determine batch size\")\n",
    "\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        final_attention_mask = None\n",
    "        final_embeds = None\n",
    "\n",
    "        # 이미지 처리\n",
    "        if pixel_values is not None:\n",
    "            # [batch_size, num_patches + 1, hidden_size]\n",
    "            image_embeds = self.patch_embed(pixel_values)\n",
    "            image_attention_mask = torch.ones(\n",
    "                (batch_size, image_embeds.shape[1]),\n",
    "                dtype=torch.long,\n",
    "                device=image_embeds.device\n",
    "            )\n",
    "            final_embeds = image_embeds  # [batch_size, num_patches + 1, hidden_size]\n",
    "            final_attention_mask = image_attention_mask  # [batch_size, num_patches + 1]\n",
    "\n",
    "        # 텍스트 처리 (선택적)\n",
    "        if input_ids is not None or inputs_embeds is not None:\n",
    "            if inputs_embeds is None:\n",
    "                # [batch_size, seq_len] -> [batch_size, seq_len, hidden_size]\n",
    "                inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "            text_attention_mask = attention_mask if attention_mask is not None else torch.ones(\n",
    "                (batch_size, inputs_embeds.shape[1]),\n",
    "                dtype=torch.long,\n",
    "                device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "            # 이미지가 있는 경우 텍스트와 연결\n",
    "            if final_embeds is not None:\n",
    "                # [batch_size, num_patches + 1 + seq_len, hidden_size]\n",
    "                final_embeds = torch.cat([final_embeds, inputs_embeds], dim=1)\n",
    "                # [batch_size, num_patches + 1 + seq_len]\n",
    "                final_attention_mask = torch.cat([final_attention_mask, text_attention_mask], dim=1)\n",
    "            else:\n",
    "                final_embeds = inputs_embeds  # [batch_size, seq_len, hidden_size]\n",
    "                final_attention_mask = text_attention_mask  # [batch_size, seq_len]\n",
    "\n",
    "        if final_embeds is None:\n",
    "            raise ValueError(\"Either pixel_values or input_ids/inputs_embeds must be provided\")\n",
    "\n",
    "        # TTT 모델 실행\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=final_attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=final_embeds,\n",
    "            cache_params=cache_params,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        # [batch_size, total_seq_len, hidden_size]\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # 로짓 계산\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)  # [batch_size, total_seq_len, vocab_size]\n",
    "        else:\n",
    "            # [batch_size, total_seq_len, vocab_size]\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        # Loss 계산\n",
    "        loss = None\n",
    "        if labels is not None and (input_ids is not None or inputs_embeds is not None):\n",
    "            text_start = self.config.num_patches + 1 if pixel_values is not None else 0\n",
    "            # [batch_size, seq_len - 1, vocab_size]\n",
    "            shift_logits = logits[:, text_start:-1, :].contiguous()\n",
    "            # [batch_size, seq_len - 1]\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "            shift_labels = shift_labels.view(-1)  # [batch_size * (seq_len - 1)]\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return TTTCausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,  # [batch_size, total_seq_len, vocab_size]\n",
    "            cache_params=outputs.cache_params,\n",
    "            hidden_states=outputs.hidden_states\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            cache_params: Optional[Any] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        model_inputs = {}\n",
    "\n",
    "        # 캐시가 있는 경우 마지막 토큰만 처리\n",
    "        if cache_params is not None:\n",
    "            if input_ids is not None:\n",
    "                input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask[:, -1].unsqueeze(-1)\n",
    "            if inputs_embeds is not None:\n",
    "                inputs_embeds = inputs_embeds[:, -1:, :]\n",
    "\n",
    "        # 입력 설정\n",
    "        if inputs_embeds is not None and cache_params is None:\n",
    "            model_inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "        elif input_ids is not None:\n",
    "            model_inputs[\"input_ids\"] = input_ids\n",
    "\n",
    "        # 첫 forward pass에서만 이미지 처리\n",
    "        if pixel_values is not None and cache_params is None:\n",
    "            model_inputs[\"pixel_values\"] = pixel_values\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"cache_params\": cache_params,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def _update_model_kwargs_for_generation(\n",
    "            self,\n",
    "            outputs: ModelOutput,\n",
    "            model_kwargs: Dict[str, Any],\n",
    "            **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        model_kwargs[\"cache_params\"] = outputs.get(\"cache_params\", None)\n",
    "\n",
    "        if \"attention_mask\" in model_kwargs:\n",
    "            attention_mask = model_kwargs[\"attention_mask\"]\n",
    "            if attention_mask is not None:\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "        # 첫 번째 forward pass 이후에는 이미지 처리 불필요\n",
    "        if \"pixel_values\" in model_kwargs:\n",
    "            del model_kwargs[\"pixel_values\"]\n",
    "\n",
    "        return model_kwargs"
   ],
   "id": "aa34528aea2c2c8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "model = TTTForVisionCausalLM(config=TTTVisionConfig(\n",
    "    **common_config, num_classes=len(train_dataset.classes), mini_batch_size=17\n",
    "))\n",
    "model.to(device)"
   ],
   "id": "9c87884c1ea1829b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "2f914f6148a87ae7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    train_line, = ax.plot(train_losses, label=\"Train Loss\", color=\"purple\")\n",
    "    valid_line, = ax.plot(valid_losses, label=\"Valid Loss\", color=\"red\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(train_loss=None, valid_loss=None):\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss)\n",
    "        if valid_loss is not None:\n",
    "            valid_losses.append(valid_loss)\n",
    "        train_line.set_ydata(train_losses)\n",
    "        train_line.set_xdata(range(len(train_losses)))\n",
    "        valid_line.set_ydata(valid_losses)\n",
    "        valid_line.set_xdata(range(len(valid_losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "id": "6d22e15544d20781",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ],
   "id": "967ecd5b0e916b90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_CACHE = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "id": "42dd00e380308053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normal Vi-T",
   "id": "ca59696e6896b548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_acc, train_loss = 0, 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() / train_length\n",
    "            corrects = (torch.max(outputs, 1)[1] == targets.data).sum()\n",
    "            train_acc += corrects / len(train_dataset)\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'Acc': corrects/len(inputs)*100, 'Loss': loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:2}/{train_length}], Acc: {corrects/len(inputs):.6%}, Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {train_acc:.6%}, Loss: {train_loss:.6f}\", end=\"\")\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / (len(inputs) * valid_length)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        update(train_loss=train_loss, valid_loss=val_loss)\n",
    "        wandb.log({'Train Acc': train_acc*100, 'Train Loss': train_loss, 'Val Acc': val_acc*100, 'Val Loss': val_loss})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {train_acc:.6%}, Loss: {train_loss:.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "id": "581a23258c46931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    import os\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"normal_vit_model.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "8a33998508f5da6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TTT Vi-T",
   "id": "3e3e79da650eb914"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "    tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(pixel_values=inputs, labels=targets, use_cache=False).logits[:, -1, :]\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            if MAX_GRAD_NORM > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'Acc': avg(train_acc)*100, 'Loss': avg(train_loss)})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:2}/{train_length}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}\", end=\"\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        cache_params = None  # Save the training cache state\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                if USE_CACHE:\n",
    "                    outputs = model(\n",
    "                        pixel_values=inputs,\n",
    "                        labels=targets,  # Pass labels to compute model's internal loss\n",
    "                        cache_params=cache_params,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = model(pixel_values=inputs, labels=targets, use_cache=False)\n",
    "                outputs, cache_params = outputs.logits[:, -1, :], outputs.cache_params\n",
    "\n",
    "                val_loss.append(criterion(outputs, targets).item())\n",
    "                val_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        update(train_loss=avg(train_loss), valid_loss=avg(val_loss))\n",
    "        wandb.log({'Train Acc': avg(train_acc)*100, 'Train Loss': avg(train_loss), 'Val Acc': avg(val_acc)*100, 'Val Loss': avg(val_loss)})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}, Valid Acc: {avg(val_acc):.6%}, Valid Loss: {avg(val_loss):.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "id": "ce7fd351c87211b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    import os\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"ttt_vit_model.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "f2a40cce237a6e53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# WandB Initialization\n",
    "wandb.init(project=\"cifar10_pytorch\")\n",
    "#wandb.init(project=\"cifar10_pytorch_ttt\")"
   ],
   "id": "99804dd3727b56e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Evaluation",
   "id": "7762d126105c4809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Model\n",
    "model_id = \"normal_vit_model\"\n",
    "\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "id": "435a4c75ebeb969d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "id": "9faf5249a208f03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(pixel_values=inputs, labels=targets, use_cache=False)\n",
    "        _, preds = torch.max(outputs.logits[:, -1, :], 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "id": "c05d535d69419f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9950465b4726834f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
