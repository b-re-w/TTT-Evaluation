{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "<!--* freshness: { reviewed: '2024-06-13' } *-->\n",
    "\n",
    "**JAX a library for array-oriented numerical computation (*Ã  la*\n",
    "[NumPy](https://numpy.org/)), with automatic differentiation and JIT\n",
    "compilation to enable high-performance machine learning research**.\n",
    "\n",
    "This document provides a quick overview of essential JAX features, so\n",
    "you can get started with JAX quickly:\n",
    "\n",
    "-   JAX provides a unified NumPy-like interface to computations that run\n",
    "    on CPU, GPU, or TPU, in local or distributed settings.\n",
    "-   JAX features built-in Just-In-Time (JIT) compilation via [Open\n",
    "    XLA](https://github.com/openxla), an open-source machine learning\n",
    "    compiler ecosystem.\n",
    "-   JAX functions support efficient evaluation of gradients via its\n",
    "    automatic differentiation transformations.\n",
    "-   JAX functions can be automatically vectorized to efficiently map\n",
    "    them over arrays representing batches of inputs."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "JAX can be installed for CPU on Linux, Windows, and macOS directly from\n",
    "the [Python Package Index](https://pypi.org/project/jax/):\n",
    "\n",
    "    pip install jax\n",
    "\n",
    "or, for NVIDIA GPU:\n",
    "\n",
    "    pip install -U \"jax[cuda12]\"\n",
    "\n",
    "For more detailed platform-specific installation information, check out\n",
    "{ref}`installation`."
   ],
   "id": "7a7e860dc422c47c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## JAX as NumPy\n",
    "\n",
    "Most JAX usage is through the familiar {mod}`jax.numpy` API, which is\n",
    "typically imported under the `jnp` alias:"
   ],
   "id": "9502154abda02144"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:08:54.264773Z",
     "start_time": "2024-09-19T02:08:52.841757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.devices()"
   ],
   "id": "c7cded7fef6fe46f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With this import, you can immediately use JAX in a similar manner to\n",
    "typical NumPy programs, including using NumPy-style array creation\n",
    "functions, Python functions and operators, and array attributes and\n",
    "methods:"
   ],
   "id": "8a968697aaafbedd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:08:54.735895Z",
     "start_time": "2024-09-19T02:08:54.267661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(selu(x))"
   ],
   "id": "3d837ba325a59221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.05      2.1       3.1499999 4.2      ]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Youâ€™ll find a few differences between JAX arrays and NumPy arrays once\n",
    "you begin digging-in; these are explored in [ðŸ”ª JAX - The Sharp Bits\n",
    "ðŸ”ª](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ],
   "id": "fe69b29bcae84af4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Just-in-time compilation with {func}`jax.jit`\n",
    "\n",
    "JAX runs transparently on the GPU or TPU (falling back to CPU if you\n",
    "donâ€™t have one). However, in the above example, JAX is dispatching\n",
    "kernels to the chip one operation at a time. If we have a sequence of\n",
    "operations, we can use the {func}`jax.jit` function to compile this\n",
    "sequence of operations together using XLA.\n",
    "\n",
    "We can use IPythonâ€™s `%timeit` to quickly benchmark our `selu` function,\n",
    "using `block_until_ready()` to account for JAXâ€™s dynamic dispatch (See\n",
    "{ref}`async-dispatch`):"
   ],
   "id": "ff017ab96fa849dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:09:27.134441Z",
     "start_time": "2024-09-19T02:08:58.642656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(1701)\n",
    "x = random.normal(key, (1_000_000,))\n",
    "%timeit selu(x).block_until_ready()"
   ],
   "id": "11c1ec4c513b8b31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.81 ms Â± 857 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "(notice weâ€™ve used {mod}`jax.random` to generate some random numbers;\n",
    "for details on how to generate random numbers in JAX, check out\n",
    "{ref}`pseudorandom-numbers`).\n",
    "\n",
    "We can speed the execution of this function with the {func}`jax.jit`\n",
    "transformation, which will jit-compile the first time `selu` is called\n",
    "and will be cached thereafter."
   ],
   "id": "117d713c2774a70a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:11:53.469104Z",
     "start_time": "2024-09-19T02:11:42.153423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import jit\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "_ = selu_jit(x)  # compiles on first call\n",
    "%timeit selu_jit(x).block_until_ready()"
   ],
   "id": "b0f397276175139e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 ms Â± 11.1 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above timing represent execution on CPU, but the same code can be\n",
    "run on GPU or TPU, typically for an even greater speedup.\n",
    "\n",
    "For more on JIT compilation in JAX, check out {ref}`jit-compilation`."
   ],
   "id": "d663dedaa918d3c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Taking derivatives with {func}`jax.grad`\n",
    "\n",
    "In addition to transforming functions via JIT compilation, JAX also\n",
    "provides other transformations. One such transformation is\n",
    "{func}`jax.grad`, which performs [automatic differentiation\n",
    "(autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation):"
   ],
   "id": "c77e301a777105f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:11:56.970209Z",
     "start_time": "2024-09-19T02:11:56.605946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import grad\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ],
   "id": "3b8b2253e50f46d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499357]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Letâ€™s verify with finite differences that our result is correct.",
   "id": "25af64bb36e73f01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:11:58.046236Z",
     "start_time": "2024-09-19T02:11:57.486682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def first_finite_differences(f, x, eps=1E-3):\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                      for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ],
   "id": "7cee82bdede6d16f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The {func}`~jax.grad` and {func}`~jax.jit` transformations compose and\n",
    "can be mixed arbitrarily. In the above example we jitted `sum_logistic`\n",
    "and then took its derivative. We can go further:"
   ],
   "id": "642ff0e37353190b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:03.730201Z",
     "start_time": "2024-09-19T02:12:03.561777Z"
    }
   },
   "cell_type": "code",
   "source": "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))",
   "id": "15fda8c659124405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0353256\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Beyond scalar-valued functions, the {func}`jax.jacobian` transformation\n",
    "can be used to compute the full Jacobian matrix for vector-valued\n",
    "functions:"
   ],
   "id": "569cd745b8fdc9b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:04.830746Z",
     "start_time": "2024-09-19T02:12:04.740921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import jacobian\n",
    "print(jacobian(jnp.exp)(x_small))"
   ],
   "id": "f76b84a84f7218c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        2.7182817 0.       ]\n",
      " [0.        0.        7.389056 ]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For more advanced autodiff operations, you can use {func}`jax.vjp` for\n",
    "reverse-mode vector-Jacobian products, and {func}`jax.jvp` and\n",
    "{func}`jax.linearize` for forward-mode Jacobian-vector products. The two\n",
    "can be composed arbitrarily with one another, and with other JAX\n",
    "transformations. For example, {func}`jax.jvp` and {func}`jax.vjp` are\n",
    "used to define the forward-mode {func}`jax.jacfwd` and reverse-mode\n",
    "{func}`jax.jacrev` for computing Jacobians in forward- and reverse-mode,\n",
    "respectively. Hereâ€™s one way to compose them to make a function that\n",
    "efficiently computes full Hessian matrices:"
   ],
   "id": "419c83e70e425b63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:09.682558Z",
     "start_time": "2024-09-19T02:12:09.593344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))\n",
    "print(hessian(sum_logistic)(x_small))"
   ],
   "id": "d02a2e91222cc03f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.         -0.        ]\n",
      " [-0.         -0.09085774 -0.        ]\n",
      " [-0.         -0.         -0.07996248]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This kind of composition produces efficient code in practice; this is\n",
    "more-or-less how JAXâ€™s built-in {func}`jax.hessian` function is\n",
    "implemented.\n",
    "\n",
    "For more on automatic differentiation in JAX, check out\n",
    "{ref}`automatic-differentiation`."
   ],
   "id": "75fe35323e6692bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Auto-vectorization with {func}`jax.vmap`\n",
    "\n",
    "Another useful transformation is {func}`~jax.vmap`, the vectorizing map.\n",
    "It has the familiar semantics of mapping a function along array axes,\n",
    "but instead of explicitly looping over function calls, it transforms the\n",
    "function into a natively vectorized version for better performance. When\n",
    "composed with {func}`~jax.jit`, it can be just as performant as manually\n",
    "rewriting your function to operate over an extra batch dimension.\n",
    "\n",
    "Weâ€™re going to work with a simple example, and promote matrix-vector\n",
    "products into matrix-matrix products using {func}`~jax.vmap`. Although\n",
    "this is easy to do by hand in this specific case, the same technique can\n",
    "apply to more complicated functions."
   ],
   "id": "9b059421671de85f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:13.725058Z",
     "start_time": "2024-09-19T02:12:13.092016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key1, key2 = random.split(key)\n",
    "mat = random.normal(key1, (150, 100))\n",
    "batched_x = random.normal(key2, (10, 100))\n",
    "\n",
    "def apply_matrix(x):\n",
    "    return jnp.dot(mat, x)"
   ],
   "id": "8ed20a55f6d2c968",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `apply_matrix` function maps a vector to a vector, but we may want\n",
    "to apply it row-wise across a matrix. We could do this by looping over\n",
    "the batch dimension in Python, but this usually results in poor\n",
    "performance."
   ],
   "id": "92b55a8a1291036d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:18.100989Z",
     "start_time": "2024-09-19T02:12:15.679479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ],
   "id": "cb91e9c714de6d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "2.75 ms Â± 194 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A programmer familiar with the the `jnp.dot` function might recognize\n",
    "that `apply_matrix` can be rewritten to avoid explicit looping, using\n",
    "the built-in batching semantics of `jnp.dot`:"
   ],
   "id": "563def3b81984eb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:21.856110Z",
     "start_time": "2024-09-19T02:12:20.310240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "@jit\n",
    "def batched_apply_matrix(batched_x):\n",
    "    return jnp.dot(batched_x, mat.T)\n",
    "\n",
    "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
    "                           batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ],
   "id": "a0e0830b96b47f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "1.45 ms Â± 107 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "However, as functions become more complicated, this kind of manual\n",
    "batching becomes more difficult and error-prone. The {func}`~jax.vmap`\n",
    "transformation is designed to automatically transform a function into a\n",
    "batch-aware version:"
   ],
   "id": "c9e5e9aff64d1a98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:12:41.177902Z",
     "start_time": "2024-09-19T02:12:29.802947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import vmap\n",
    "\n",
    "@jit\n",
    "def vmap_batched_apply_matrix(batched_x):\n",
    "    return vmap(apply_matrix)(batched_x)\n",
    "\n",
    "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
    "                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ],
   "id": "a0d815c93e4df5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "1.38 ms Â± 6.54 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you would expect, {func}`~jax.vmap` can be arbitrarily composed with\n",
    "{func}`~jax.jit`, {func}`~jax.grad`, and any other JAX transformation.\n",
    "\n",
    "For more on automatic vectorization in JAX, check out\n",
    "{ref}`automatic-vectorization`.\n",
    "\n",
    "This is just a taste of what JAX can do. Weâ€™re really excited to see\n",
    "what you do with it!"
   ],
   "id": "da95ba252896b0cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c18b333c777b851"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
